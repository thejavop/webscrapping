from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import pandas as pd
import time

class ABCNewsScraper:
    def __init__(self):
        """Inicializa el scraper con configuraci√≥n de Selenium"""
        # Configurar opciones de Chrome
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

        # Inicializar driver
        self.driver = webdriver.Chrome(options=options)
        self.driver.implicitly_wait(10)

        # Almacenar noticias
        self.noticias = []

        # Definir secciones a scrape (URLs de ABC.es)
        self.secciones = {
            'Internacional': 'https://www.abc.es/internacional/',
            'Economia': 'https://www.abc.es/economia/',
            'Tecnologia': 'https://www.abc.es/tecnologia/',
            'Cultura': 'https://www.abc.es/cultura/'
        }

    def aceptar_cookies(self):
        """Acepta el banner de cookies si aparece"""
        try:
            print("Buscando banner de cookies...")
            # Probar varios selectores comunes para banners de cookies en espa√±ol
            selectores_cookies = [
                "//button[contains(text(), 'Aceptar')]",
                "//button[contains(text(), 'Acepto')]",
                "//button[contains(text(), 'Accept')]",
                "//a[contains(text(), 'Aceptar')]",
                "//button[@id='didomi-notice-agree-button']"  # Didomi es com√∫n en sitios espa√±oles
            ]
            
            for selector in selectores_cookies:
                try:
                    boton_cookies = WebDriverWait(self.driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, selector))
                    )
                    boton_cookies.click()
                    print("‚úì Cookies aceptadas")
                    time.sleep(1)
                    return
                except:
                    continue
                    
            print("No se encontr√≥ banner de cookies")
        except Exception as e:
            print(f"Error con cookies: {e}")

    def scroll_pagina(self, scrolls=3):
        """Hace scroll para cargar m√°s contenido din√°mico"""
        print(f"Haciendo scroll ({scrolls} veces)...")
        for i in range(scrolls):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            print(f"   Scroll {i+1}/{scrolls}")
    
    def extraer_articulos(self, max_articulos=100):
        """Extrae t√≠tulos y descripciones de art√≠culos de la p√°gina actual"""
        articulos_extraidos = []

        try:
            # Selectores comunes en sitios de noticias espa√±oles
            # NOTA: Estos selectores son APROXIMADOS, tendr√°s que ajustarlos
            selectores = [
                "article",
                "div[class*='noticia']",
                "div[class*='article']",
                "div[class*='news']",
                "div[class*='story']",
                "div[data-test*='article']",
                "li[class*='item']"
            ]

            articulos_elementos = []
            for selector in selectores:
                try:
                    elementos = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elementos and len(elementos) > 5:  # Al menos 5 elementos
                        articulos_elementos = elementos
                        print(f"‚úì Encontrados {len(elementos)} elementos con selector: {selector}")
                        break
                except:
                    continue

            if not articulos_elementos:
                print("‚ö†Ô∏è  No se encontraron art√≠culos con ning√∫n selector")
                # Imprimir HTML para debugging
                print("\n--- HTML DE LA P√ÅGINA (primeros 500 caracteres) ---")
                print(self.driver.page_source[:500])
                return articulos_extraidos
            
            self.driver.implicitly_wait(0)

            articulos_elementos = articulos_elementos[:max_articulos]
            print(f"Procesando {len(articulos_elementos)} elementos...")

            for idx, elemento in enumerate(articulos_elementos, 1):
                try:
                    if idx % 10 == 0:
                        print(f"   Procesados {idx}/{len(articulos_elementos)} elementos... ({len(articulos_extraidos)} art√≠culos v√°lidos)")

                    titulo = None
                    # Buscar t√≠tulo con varios selectores
                    try:
                        # Probar h1, h2, h3, h4
                        for tag in ['h1', 'h2', 'h3', 'h4']:
                            try:
                                titulo_elem = elemento.find_element(By.TAG_NAME, tag)
                                titulo = titulo_elem.text.strip()
                                if titulo and len(titulo) > 10:
                                    break
                            except:
                                continue
                        
                        # Si no encontr√≥ t√≠tulo, probar con clase title
                        if not titulo:
                            titulo_elem = elemento.find_element(By.CSS_SELECTOR, "[class*='title'], [class*='titulo'], [class*='headline']")
                            titulo = titulo_elem.text.strip()
                    except:
                        pass

                    descripcion = None
                    try:
                        # Buscar descripci√≥n/sumario
                        desc_elem = elemento.find_element(By.CSS_SELECTOR, "p, [class*='description'], [class*='descripcion'], [class*='summary'], [class*='sumario']")
                        descripcion = desc_elem.text.strip()
                    except:
                        pass

                    if titulo and len(titulo) > 10:
                        articulos_extraidos.append({
                            'titulo': titulo,
                            'descripcion': descripcion if descripcion else ""
                        })

                except Exception as e:
                    continue

            print(f"   Procesamiento completado: {len(articulos_elementos)} elementos")

            # Eliminar duplicados
            titulos_vistos = set()
            articulos_unicos = []
            for articulo in articulos_extraidos:
                if articulo['titulo'] not in titulos_vistos:
                    titulos_vistos.add(articulo['titulo'])
                    articulos_unicos.append(articulo)

            return articulos_unicos[:max_articulos]

        except Exception as e:
            print(f"Error extrayendo art√≠culos: {e}")
            return articulos_extraidos
    
    def scrapear_seccion(self, nombre_categoria, url, max_articulos=200):
        """Scrape una secci√≥n espec√≠fica de ABC"""
        print(f"\n{'='*60}")
        print(f"Scrapeando: {nombre_categoria}")
        print(f"URL: {url}")
        print(f"{'='*60}")

        articulos_categoria = []
        pagina = 1
        max_paginas = 10  # Aumentado a 10 p√°ginas

        try:
            while len(articulos_categoria) < max_articulos and pagina <= max_paginas:
                print(f"\n{'*'*40}")
                print(f"P√ÅGINA {pagina}/{max_paginas}")
                print(f"Art√≠culos acumulados: {len(articulos_categoria)}")
                print(f"{'*'*40}")

                # SOLO cargar URL en la primera p√°gina
                if pagina == 1:
                    self.driver.get(url)
                    time.sleep(3)
                    
                    if len(self.noticias) == 0:
                        self.aceptar_cookies()

                self.scroll_pagina(scrolls=5)

                articulos = self.extraer_articulos(max_articulos - len(articulos_categoria))
                
                print(f"\n>>> Extra√≠dos de la p√°gina: {len(articulos)} art√≠culos")
                
                # Mostrar primeros 3 t√≠tulos para verificar
                print("\nüì∞ PRIMEROS 3 T√çTULOS:")
                for i, art in enumerate(articulos[:3], 1):
                    print(f"   {i}. {art['titulo'][:80]}...")

                # Contar duplicados
                nuevos = 0
                duplicados = 0
                
                for articulo in articulos:
                    if articulo['titulo'] not in [a['titulo'] for a in articulos_categoria]:
                        articulos_categoria.append(articulo)
                        nuevos += 1
                    else:
                        duplicados += 1

                print(f"\n>>> Nuevos agregados: {nuevos}")
                print(f">>> Duplicados ignorados: {duplicados}")
                print(f">>> TOTAL ACUMULADO: {len(articulos_categoria)} art√≠culos")

                # Si no hay art√≠culos nuevos, parar
                if nuevos == 0:
                    print("\n‚ö†Ô∏è  NO SE AGREGARON ART√çCULOS NUEVOS - Fin de paginaci√≥n")
                    break

                # Estrategia: Bot√≥n "Cargar m√°s" / "Ver m√°s"
                try:
                    print("\nüîÑ Buscando bot√≥n 'Cargar m√°s'...")
                    
                    load_more_button = None
                    
                    # Selectores para bot√≥n de cargar m√°s
                    selectores_load_more = [
                        "div.voc-btn__container button",  # Selector espec√≠fico de ABC
                        "div.voc-btn__container a",
                        "button[class*='cargar']",
                        "button[class*='more']",
                        "a[class*='cargar']",
                        "a[class*='more']",
                        "//button[contains(text(), 'Cargar')]",
                        "//button[contains(text(), 'Ver m√°s')]",
                        "//a[contains(text(), 'Cargar')]",
                        "//a[contains(text(), 'Ver m√°s')]"
                    ]
                    
                    # Primero hacer scroll hacia abajo para que el bot√≥n sea visible
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                    
                    for selector in selectores_load_more:
                        try:
                            if selector.startswith("//"):
                                load_more_button = self.driver.find_element(By.XPATH, selector)
                            else:
                                load_more_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                            
                            # Verificar que el bot√≥n es visible
                            if load_more_button.is_displayed():
                                print(f"‚úì Bot√≥n 'Cargar m√°s' encontrado con: {selector}")
                                break
                            else:
                                load_more_button = None
                        except:
                            continue
                    
                    if load_more_button:
                        try:
                            # Scroll al bot√≥n para asegurarnos que est√° visible
                            self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", load_more_button)
                            time.sleep(1)
                            
                            # Click con JavaScript (m√°s fiable)
                            self.driver.execute_script("arguments[0].click();", load_more_button)
                            print(f"‚úì Click en 'Cargar m√°s' ejecutado")
                            
                            # Esperar a que carguen los nuevos art√≠culos
                            time.sleep(4)  # Dar tiempo para que carguen
                            
                            # Hacer scroll adicional para cargar el contenido nuevo
                            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                            time.sleep(2)
                            
                            pagina += 1
                            print(f"‚úì P√°gina {pagina} cargada")
                            
                        except Exception as e:
                            print(f"Error haciendo click en bot√≥n: {e}")
                            break
                    else:
                        print("‚úó No se encontr√≥ bot√≥n 'Cargar m√°s' - fin de contenido")
                        break
                        
                except Exception as e:
                    print(f"Error en carga de m√°s art√≠culos: {e}")
                    break

            # A√±adir categor√≠a a cada art√≠culo
            for articulo in articulos_categoria:
                articulo['categoria'] = nombre_categoria
                self.noticias.append(articulo)

            print(f"\n{'='*60}")
            print(f"RESUMEN {nombre_categoria}:")
            print(f"Total extra√≠do: {len(articulos_categoria)} art√≠culos")
            print(f"P√°ginas procesadas: {pagina}")
            print(f"Total acumulado: {len(self.noticias)} art√≠culos")
            print(f"{'='*60}")

            time.sleep(2)  # Delay entre secciones

        except Exception as e:
            print(f"Error scrapeando {nombre_categoria}: {e}")
    
    def scrapear_todas_secciones(self, max_por_categoria=200):
        """Scrape todas las secciones definidas"""
        print("="*60)
        print("INICIANDO SCRAPING DE ABC.es")
        print("="*60)
        print(f"Objetivo: {max_por_categoria} art√≠culos por categor√≠a")
        print(f"Categor√≠as: {list(self.secciones.keys())}\n")

        for categoria, url in self.secciones.items():
            self.scrapear_seccion(categoria, url, max_por_categoria)

        print(f"\n{'='*60}")
        print(f"SCRAPING COMPLETADO")
        print(f"Total de art√≠culos: {len(self.noticias)}")
        print(f"{'='*60}")
    
    def guardar_datos(self, filename='abc_news.csv'):
        """Guarda los datos en un CSV"""
        if not self.noticias:
            print("No hay noticias para guardar")
            return None

        df = pd.DataFrame(self.noticias)

        # Limpiar datos
        df = df.drop_duplicates(subset=['titulo'])
        df = df[df['titulo'].str.len() > 10]

        df.to_csv(filename, index=False, encoding='utf-8')

        print(f"\n‚úì Datos guardados en '{filename}'")
        print(f"\nRESUMEN FINAL:")
        print(f"   - Total art√≠culos: {len(df)}")
        print(f"   - Distribuci√≥n por categor√≠a:")
        print(df['categoria'].value_counts().to_string())
        print(f"\nMuestra de datos:")
        print(df.head(3).to_string())

        return df

    def cerrar(self):
        """Cierra el navegador"""
        print("\nCerrando navegador...")
        self.driver.quit()


if __name__ == "__main__":
    scraper = ABCNewsScraper()

    try:
        scraper.scrapear_todas_secciones(max_por_categoria=200)
        df = scraper.guardar_datos('abc_news.csv')

        if df is not None:
            print("\n‚úì Proceso completado exitosamente")
            print(f"‚úì Archivo generado: abc_news.csv")

    except KeyboardInterrupt:
        print("\nScraping interrumpido por el usuario")

    except Exception as e:
        print(f"\nError general: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.cerrar()